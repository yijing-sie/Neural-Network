{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "HVzR_BNP1dF3",
      "metadata": {
        "id": "HVzR_BNP1dF3"
      },
      "source": [
        "# Read me\n",
        "1. To successfully run this code, one has to put all the **npy** files under the folder where this code is being run.\n",
        "2. Ceate a folder called **test** where all the pth files will be stored.\n",
        "3. Run cells one after another to train and test the model\n",
        "\n",
        "## Model Description\n",
        "This model is compsed of 10 layers with *bach normalization* performmed before each *CrossEntropyLoss* activation layer, and the optimizer for gradient descent optimization is Adam.\n",
        "\n",
        "\n",
        "\n",
        "Details about the model structure is written in a text cell above the corresponding code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965fc738-030c-47ea-9391-a4536df5f2a4",
      "metadata": {
        "id": "965fc738-030c-47ea-9391-a4536df5f2a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c35946a-1e2e-4096-892b-628f05e99776",
      "metadata": {
        "id": "8c35946a-1e2e-4096-892b-628f05e99776"
      },
      "source": [
        "There are three reasons why I set context to be 25:\n",
        "1. On the piazza post [@411](https://colab.research.google.com/drive/1KAcru1O0asbOrY12v2fBl345_Eynrtbd?authuser=2#scrollTo=8c35946a-1e2e-4096-892b-628f05e99776), it explicitly states that when context = 20, the model can easily help you reach B cutoff. \n",
        "2. The writeup for p2 says that the recommended value of context is between 5-30, and because the value of context determines the portion of speech preceding and succeeding one sound, when we have a higher value of context, one input contains more information for classification, which can lead to better performance. \n",
        "3. Since context = 20 is the recommended value for reaching B cutoff, to reach A cutoff, the value of context must be increased, and because the maximal recommended value is 30, I simply tried the midpoint between 20 and 30, which is 25, for my model, which indeed boosts the classification performance for my model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3273490a-c8a9-430d-8d90-38db87ee9fe1",
      "metadata": {
        "id": "3273490a-c8a9-430d-8d90-38db87ee9fe1"
      },
      "outputs": [],
      "source": [
        "#%%setting offset and context\n",
        "context = 25\n",
        "offset = context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b110a266-bed5-4636-91cd-381c050bdda2",
      "metadata": {
        "id": "b110a266-bed5-4636-91cd-381c050bdda2"
      },
      "outputs": [],
      "source": [
        "#%%Training data_set\n",
        "class Train_Dataset(torch.utils.data.Dataset):    \n",
        "    def __init__(self, X, Y, offset = offset, context = context):\n",
        "        \n",
        "        # Add data and label to self \n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        \n",
        "        #data index mapping \n",
        "        index_map_X = []\n",
        "        for i, x in enumerate(X):\n",
        "            for j, xx in enumerate(x):\n",
        "                index_pair_X = (i, j)\n",
        "                index_map_X.append(index_pair_X)\n",
        "        \n",
        "        #Assign data index mapping to self \n",
        "        self.index_map = index_map_X\n",
        "        \n",
        "        #Add length to self \n",
        "        self.length = len(index_map_X)\n",
        "        \n",
        "        #Add context and offset to self \n",
        "        self.context = context\n",
        "        self.offset = offset\n",
        "        \n",
        "        #Zero pad data as-needed for context \n",
        "        for i, x in enumerate(self.X):\n",
        "            self.X[i] = np.pad(x, ((context, context), (0, 0)), 'constant', constant_values=0)\n",
        "        \n",
        "    def __len__(self):\n",
        "        \n",
        "        #Return length \n",
        "        return self.length\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        #Get index pair from index map \n",
        "        i, j = self.index_map[index]\n",
        "        \n",
        "        #Calculate starting timestep using offset and context \n",
        "        start_j = j + self.offset - self.context\n",
        "        \n",
        "        #Calculate ending timestep using offset and context \n",
        "        end_j = j + self.offset + self.context + 1\n",
        "        \n",
        "        #Get data at index pair with context \n",
        "        features = self.X[i][start_j:end_j, :]\n",
        "        \n",
        "        #Get label at index pair\n",
        "        labels = self.Y[i][j]\n",
        "        \n",
        "        ### Return data at index pair with context and label at index pair \n",
        "        return features, labels\n",
        "    \n",
        "    def collate_fn(batch):\n",
        "        \n",
        "        ### Select all data from batch \n",
        "        batch_x = [x for x, y in batch]\n",
        "        \n",
        "        ### Select all labels from batch\n",
        "        batch_y = [y for x, y in batch]\n",
        "        \n",
        "        ### Convert batched data and labels to tensors \n",
        "        batch_x = torch.as_tensor(batch_x)\n",
        "        batch_y = torch.as_tensor(batch_y)\n",
        "        \n",
        "        ### Return batched data and labels \n",
        "        return batch_x, batch_y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eVWsLcZ2W_-F",
      "metadata": {
        "id": "eVWsLcZ2W_-F"
      },
      "source": [
        "After doing some research, I found this article talking about ways to make Pytorch train faster\n",
        ", [Faster Deep Learning Training with PyTorch â€“ a 2021 Guide](https://efficientdl.com/faster-deep-learning-in-pytorch-a-guide/). This article says that a 2x speed-up for a single training epoch by using four workers and pinned memory, and that's why I added them in the codes for both of my dataloaders.\n",
        "\n",
        "The value of batch size is according to the piazza post [@411](https://colab.research.google.com/drive/1KAcru1O0asbOrY12v2fBl345_Eynrtbd?authuser=2#scrollTo=8c35946a-1e2e-4096-892b-628f05e99776)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa44e7d-749e-4291-b915-0f8064d2d8e1",
      "metadata": {
        "id": "2fa44e7d-749e-4291-b915-0f8064d2d8e1"
      },
      "outputs": [],
      "source": [
        "#%% load training and validating data\n",
        "print('loading data...')\n",
        "X = np.load(\"train.npy\", allow_pickle=True)\n",
        "y = np.load(\"train_labels.npy\", allow_pickle=True)\n",
        "val_X = np.load(\"dev.npy\", allow_pickle=True)\n",
        "val_Y = np.load(\"dev_labels.npy\", allow_pickle=True)\n",
        "train_dataset = Train_Dataset(X, y)\n",
        "#num_worker = 4 * num_GPU .\n",
        "#pin_memory = True : allocates the samples in page-locked memory, which speeds-up the transfer\n",
        "batch_size = 128\n",
        "train_dataloader  = torch.utils.data.DataLoader(train_dataset, \n",
        "                        batch_size = batch_size, \n",
        "                        shuffle=True, \n",
        "                        collate_fn= Train_Dataset.collate_fn,\n",
        "                        num_workers = 4,\n",
        "                        pin_memory = True)\n",
        "\n",
        "val_dataset = Train_Dataset(val_X, val_Y)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, \n",
        "                        batch_size = batch_size,\n",
        "                        shuffle=False, \n",
        "                        num_workers = 4,                       \n",
        "                        collate_fn= Train_Dataset.collate_fn,\n",
        "                        pin_memory = True)\n",
        "print('Data loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QpG6GniIaQ91",
      "metadata": {
        "id": "QpG6GniIaQ91"
      },
      "source": [
        "I found another [article](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) talking about not relying batchnorm for avoiding overfitting, so I add drop out layers after activations.\n",
        "\n",
        "For the value of dropout, after discussing with some of my friends who also take this course, we concluded that strating with a high value of drop out rate such as 0.5 ~ 0.8 and then decreasing it after several layers can lead to a better performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d3b2d8-6e2e-4cf6-af1c-2d33685bdca5",
      "metadata": {
        "id": "61d3b2d8-6e2e-4cf6-af1c-2d33685bdca5"
      },
      "outputs": [],
      "source": [
        "#%%neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hiddens, activation):\n",
        "        super().__init__()\n",
        "        in_h_out_0 = [input_size] + hiddens\n",
        "        in_h_out_1 =  hiddens + [output_size]\n",
        "        zip_hiddens = list(zip(in_h_out_0, in_h_out_1))\n",
        "        layers = [nn.Flatten()]\n",
        "        for i in range(len(zip_hiddens)):\n",
        "            layers.append(nn.Linear(zip_hiddens[i][0], zip_hiddens[i][1]))\n",
        "            layers.append(nn.BatchNorm1d(zip_hiddens[i][1]))\n",
        "            layers.append(activation)\n",
        "            #decreasing Droupout layers by layers large 0.5-> small  0.1\n",
        "            if i < 2: layers.append(nn.Dropout(0.5)) \n",
        "            else : layers.append(nn.Dropout(0.1)) \n",
        "        self.seq_layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.seq_layers(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gX8aFBbAnEhW",
      "metadata": {
        "id": "gX8aFBbAnEhW"
      },
      "source": [
        "* The value of learning rate is set according to this piazza post [@411](https://colab.research.google.com/drive/1KAcru1O0asbOrY12v2fBl345_Eynrtbd?authuser=2#scrollTo=8c35946a-1e2e-4096-892b-628f05e99776)\n",
        "\n",
        "* The size of hidden layers is basicly determined by try-and-error, and it is also based on the post [@411](https://colab.research.google.com/drive/1KAcru1O0asbOrY12v2fBl345_Eynrtbd?authuser=2#scrollTo=8c35946a-1e2e-4096-892b-628f05e99776)\n",
        "* The choice of loss function is according to the open source, [PYTORCH TUTORIALS_optimization](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html), whose task is also doing classification.\n",
        "* I chose Adam for optimizer because it is might be the best overall choice according to this [article](https://ruder.io/optimizing-gradient-descent/), *An overview of gradient descent optimization algorithms.*\n",
        "* I kept adding the number of layers until the valuation accuracy is at least 0.79; the size of each layers is referenced to this post [@411](https://colab.research.google.com/drive/1KAcru1O0asbOrY12v2fBl345_Eynrtbd?authuser=2#scrollTo=8c35946a-1e2e-4096-892b-628f05e99776)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "580791a8-d30d-4b4c-8eb1-3cb51516d0aa",
      "metadata": {
        "id": "580791a8-d30d-4b4c-8eb1-3cb51516d0aa"
      },
      "outputs": [],
      "source": [
        "#%% Set model parameters\n",
        "learning_rate = 1e-3\n",
        "activation = nn.LeakyReLU(0.1) \n",
        "input_size = (1 + 2 * context) * 40 \n",
        "output_size = 70\n",
        "hiddens = [2048,2048,1024,1024,512,512,256,256,128]\n",
        "#%%Initialize model\n",
        "#recommended to move a model to GPU before constructing an optimizer\n",
        "model = NeuralNetwork(input_size= input_size, output_size= output_size, \n",
        "                      hiddens= hiddens, activation= activation).to(device)\n",
        "#Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3cf960d-2c3c-4165-bac5-247146d0e8fb",
      "metadata": {
        "id": "f3cf960d-2c3c-4165-bac5-247146d0e8fb"
      },
      "outputs": [],
      "source": [
        "#%%trainning\n",
        "def training(dataloader, model, loss_fn, optimizer):\n",
        "    sum_loss, accuracy = 0.0, 0.0\n",
        "    n_batches = len(dataloader) #number of batches\n",
        "    #train mode\n",
        "    model.train()\n",
        "    for (X, y) in dataloader:\n",
        "        #sending data to device\n",
        "        X, y = X.float().to(device), y.long().to(device)\n",
        "        #Forward\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(X)\n",
        "        loss = loss_fn(prediction, y)\n",
        "\n",
        "        # Backpropagation        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        sum_loss += loss.item()  \n",
        "        y_hat = prediction.argmax(1)\n",
        "        correct = torch.sum(y_hat == y).item() / X.shape[0] #batch_size\n",
        "        accuracy += correct \n",
        "    mean_loss = sum_loss / n_batches\n",
        "    mean_accuracy = accuracy / n_batches\n",
        "    return mean_loss, mean_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20fadc7d-8811-4298-8e97-8e77929edcc7",
      "metadata": {
        "id": "20fadc7d-8811-4298-8e97-8e77929edcc7"
      },
      "outputs": [],
      "source": [
        "#%%validating\n",
        "def testing(dataloader, model, loss_fn):\n",
        "    sum_loss, accuracy = 0.0, 0.0\n",
        "    n_batches = len(dataloader) #number of batches\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for (X, y) in dataloader:\n",
        "            #sending data to device\n",
        "            X, y = X.float().to(device), y.long().to(device)\n",
        "            #Forward\n",
        "            prediction = model(X)\n",
        "            #calculating loss\n",
        "            sum_loss += loss_fn(prediction, y).item()\n",
        "            y_hat = prediction.argmax(1)            \n",
        "            correct =  torch.sum(y_hat == y).item() / X.shape[0]\n",
        "            accuracy += correct \n",
        "    mean_loss = sum_loss / n_batches\n",
        "    mean_accuracy = accuracy / n_batches\n",
        "    return mean_loss, mean_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wDn24VKiq7D0",
      "metadata": {
        "id": "wDn24VKiq7D0"
      },
      "source": [
        "I kept increasing the number of epochs until the the validation accuracy  was guaranteed to be over 0.8\n",
        "\n",
        "It took about 25~30 minutes to run one epoch, and in order to have validation accuracy over 0.8, the model has to run  at least 64 epochs.\n",
        "Therefore, it took about 2 days in total.\n",
        "\n",
        "The model is storded under a folder called **test**, so in order to successfully run this code, one has to create a new folder called **test** in the same folder where my code is being run. OR, one can modify the path where the model should be stored to avoid savepath error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0581514e-93fa-4ca3-bdb2-ac388664a3ef",
      "metadata": {
        "id": "0581514e-93fa-4ca3-bdb2-ac388664a3ef"
      },
      "outputs": [],
      "source": [
        "#%%running model\n",
        "print('Running model')\n",
        "list_loss_train, list_accuracy_train, list_loss_val, list_accuracy_val = [], [], [], []\n",
        "epochs = 70\n",
        "for e in tqdm(range(epochs)):\n",
        "    #trainning\n",
        "    train_loss, train_accuracy = training(train_dataloader, model, loss_fn, optimizer)\n",
        "    #Saving & Loading a General Checkpoint for Inference and/or Resuming Training\n",
        "    torch.save({\n",
        "            'epoch': e,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss}, '../test/'+str(e)+'.pth')\n",
        "    list_loss_train += [train_loss]\n",
        "    list_accuracy_train += [train_accuracy]\n",
        "    #valdating\n",
        "    val_loss, val_accuracy = testing(val_dataloader, model, loss_fn)\n",
        "    list_loss_val += [val_loss]\n",
        "    list_accuracy_val += [val_accuracy]\n",
        "#     loss_scheduler.step()\n",
        "    print(\"Epoch \"+str(e)+\": \\n train_loss = \" + str(train_loss) +\n",
        "          \", train_accuracy:\" + str(train_accuracy) +\n",
        "          \",\\n val_Loss:\" + str(val_loss) +\n",
        "          \", val_accuracy:\" + str(val_accuracy))\n",
        "print('Done.')\n",
        "np.savetxt(\"train_loss.csv\", list_loss_train, delimiter=\",\")\n",
        "np.savetxt(\"train_accueacy.csv\", list_accuracy_train, delimiter=\",\")\n",
        "np.savetxt(\"val_loss.csv\", list_loss_val, delimiter=\",\")\n",
        "np.savetxt(\"val_accuracy.csv\", list_accuracy_val, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c087c707-ef2f-403d-a933-9279696128a9",
      "metadata": {
        "id": "c087c707-ef2f-403d-a933-9279696128a9",
        "outputId": "4a6664d0-7209-4218-bb42-3204629f9a77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%model loading for testing\n",
        "checkpoint = torch.load('../test/'+ str(epochs - 1)+'.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ec850b0-0e44-4106-b961-126ec7ed70d5",
      "metadata": {
        "id": "3ec850b0-0e44-4106-b961-126ec7ed70d5"
      },
      "outputs": [],
      "source": [
        "#%%test data loader\n",
        "class Test_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, offset= offset, context= context):\n",
        "        \n",
        "        ### Assign data to self (1 line)\n",
        "        self.X = X\n",
        "        \n",
        "        ### Define data index mapping (4-6 lines)\n",
        "        index_map_X = []\n",
        "        \n",
        "        for i, x in enumerate(X):\n",
        "            for j, xx in enumerate(x):\n",
        "                index_pair_X = (i, j)\n",
        "                index_map_X.append(index_pair_X)\n",
        "                \n",
        "        ### Assign data index mapping to self (1 line)\n",
        "        self.index_map = index_map_X\n",
        "        \n",
        "        ### Assign length to self (1 line)\n",
        "        self.length = len(self.index_map)\n",
        "        \n",
        "        ### Add context and offset to self (1-2 line)\n",
        "        self.context = context\n",
        "        self.offset = offset\n",
        "        \n",
        "        ### Zero pad data as-needed for context size = 1 (1-2 lines)\n",
        "        for i, x in enumerate(self.X):\n",
        "            self.X[i] = np.pad(x, ((context, context), (0, 0)), 'constant', constant_values=0)\n",
        "        \n",
        "    def __len__(self):\n",
        "        \n",
        "        ### Return length (1 line)\n",
        "        return self.length\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        ### Get index pair from index map (1-2 lines)\n",
        "        i, j = self.index_map[index]\n",
        "        \n",
        "        ### Calculate starting timestep using offset and context (1 line)\n",
        "        start_j = j + self.offset - self.context\n",
        "        \n",
        "        ### Calculate ending timestep using offset and context (1 line)\n",
        "        end_j = j + self.offset + self.context + 1\n",
        "        \n",
        "        ### Get data at index pair with context (1 line)\n",
        "        feature = self.X[i][start_j:end_j,:]\n",
        "        \n",
        "        ### Return data (1 line)\n",
        "        return feature\n",
        "    \n",
        "    def collate_fn(batch):\n",
        "        \n",
        "        ### Convert batch to tensor (1 line)\n",
        "        batch_x = torch.as_tensor(batch)\n",
        "        \n",
        "        ### Return batched data and labels (1 line)\n",
        "        return batch_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30713ffe-f3b8-4dce-b0c3-a42ac4d1bb22",
      "metadata": {
        "id": "30713ffe-f3b8-4dce-b0c3-a42ac4d1bb22"
      },
      "outputs": [],
      "source": [
        "#%%loading test datd\n",
        "test_X = np.load(\"test.npy\", allow_pickle=True)\n",
        "test_X = Test_Dataset(test_X)\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "                        dataset= test_X,\n",
        "                        batch_size= 1,\n",
        "                        shuffle= False,\n",
        "                        num_workers= 4,\n",
        "                        collate_fn= Test_Dataset.collate_fn,\n",
        "                        pin_memory= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e38d0de0-28b4-462d-a1f7-85f0bcea6ed7",
      "metadata": {
        "id": "e38d0de0-28b4-462d-a1f7-85f0bcea6ed7"
      },
      "outputs": [],
      "source": [
        "#%%testing\n",
        "prediction = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for X in tqdm(test_dataloader):\n",
        "        #sending data to device\n",
        "        X = X.float().to(device)\n",
        "        #Predicting\n",
        "        prediction += [model(X).argmax(1).item()]\n",
        "#%%save prediction to csv\n",
        "prediction_df = pd.DataFrame(data = {'id':np.arange(len(prediction)), 'label': prediction})\n",
        "prediction_df.to_csv('hw1.csv', index= False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Hw1p2_yijing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
